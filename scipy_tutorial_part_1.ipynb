{"cells":[{"cell_type":"markdown","id":"728ade56","metadata":{"id":"728ade56"},"source":["# Practical Statistics for Data Science with SciPy - Part 1\n","\n","This tutorial is your hands-on guide to performing high-quality statistical analysis in Python. We'll use the powerful `pandas` library for data manipulation and `scipy` for its rich set of statistical functions. We'll also use `matplotlib` and `seaborn` for visualization, which is crucial for interpreting our results.\n","\n","**Our Goal:** To move beyond theory and learn how to ask questions, test assumptions, and find meaningful, real-world insights from data.\n","\n","---\n","\n","### Our Datasets for Today's Journey\n","\n","We will be using three distinct datasets to explore different statistical challenges:\n","\n","1.  **Telco Customer Churn (Primary Narrative):** We'll analyze why customers leave a telecom company. This is a classic business problem full of categorical and numerical data.\n","2.  **Medical Insurance Costs:** We'll explore the factors that drive up healthcare costs. This dataset is perfect for regression and correlation analysis.\n","3.  **Used Car Prices:** A large, messy dataset that will show us why we sometimes need to use more robust statistical methods when our data isn't perfect."]},{"cell_type":"code","execution_count":null,"id":"e908432d","metadata":{"id":"e908432d"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import scipy.stats as stats"]},{"cell_type":"code","execution_count":null,"id":"3404cc51","metadata":{"id":"3404cc51"},"outputs":[],"source":["# Set some display options for better readability\n","pd.set_option('display.max_columns', 50)\n","sns.set_style('whitegrid')"]},{"cell_type":"markdown","id":"ca96e5f7","metadata":{"id":"ca96e5f7"},"source":["# Module 1: Foundations - Basic Stats"]},{"cell_type":"code","execution_count":null,"id":"b05f5f13","metadata":{"id":"b05f5f13"},"outputs":[],"source":["# Sample dataset\n","data = [2, 3, 3, 5, 7, 10, 10, 10, 12, 15, 18, 20]"]},{"cell_type":"markdown","id":"6e63148c","metadata":{"id":"6e63148c"},"source":["## Measures of Central Tendency\n","\n","Measures of central tendency represent the center or typical value of a dataset."]},{"cell_type":"markdown","id":"fb3ae368","metadata":{"id":"fb3ae368"},"source":["### Mean\n","The **mean** is the sum of all values divided by the number of values. It is sensitive to outliers.\n","$$ \\text{Mean} = \\frac{\\sum_{i=1}^{n} x_i}{n} $$"]},{"cell_type":"code","execution_count":null,"id":"caa913f8","metadata":{"id":"caa913f8"},"outputs":[],"source":["mean_value = np.mean(data)\n","print(f\"Mean: {mean_value}\")"]},{"cell_type":"markdown","id":"80f292ef","metadata":{"id":"80f292ef"},"source":["### Median\n","The **median** is the middle value of a dataset that has been sorted in ascending order. It is less affected by outliers than the mean."]},{"cell_type":"code","execution_count":null,"id":"5c71a60b","metadata":{"id":"5c71a60b"},"outputs":[],"source":["median_value = np.median(data)\n","print(f\"Median: {median_value}\")"]},{"cell_type":"markdown","id":"c47b6dca","metadata":{"id":"c47b6dca"},"source":["### Mode\n","The **mode** is the value that appears most frequently in a dataset. A dataset can have one mode, more than one mode, or no mode."]},{"cell_type":"code","execution_count":null,"id":"865f8678","metadata":{"id":"865f8678"},"outputs":[],"source":["mode_value = stats.mode(data)\n","print(f\"Mode: {mode_value.mode}\")"]},{"cell_type":"markdown","id":"b1aa4871","metadata":{"id":"b1aa4871"},"source":["## Measures of Variability\n","\n","Measures of variability (or dispersion) describe the spread of the data points in a dataset."]},{"cell_type":"markdown","id":"2116fc05","metadata":{"id":"2116fc05"},"source":["### Variance\n","**Variance** measures the average squared difference of the values from the mean. A higher variance indicates that the data points are very spread out.\n","$$ \\text{Variance} (\\sigma^2) = \\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{n} $$"]},{"cell_type":"code","execution_count":null,"id":"cb7fdf7a","metadata":{"id":"cb7fdf7a"},"outputs":[],"source":["variance_value = np.var(data)\n","print(f\"Variance: {variance_value:.2f}\")"]},{"cell_type":"markdown","id":"47f6d978","metadata":{"id":"47f6d978"},"source":["### Standard Deviation\n","The **standard deviation** is the square root of the variance. It is expressed in the same units as the data, making it easier to interpret.\n","$$ \\text{Standard Deviation} (\\sigma) = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{n}} $$"]},{"cell_type":"code","execution_count":null,"id":"d4263623","metadata":{"id":"d4263623"},"outputs":[],"source":["std_dev_value = np.std(data)\n","print(f\"Standard Deviation: {std_dev_value:.2f}\")"]},{"cell_type":"code","execution_count":null,"id":"3d033f16","metadata":{"id":"3d033f16"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"7f6c7a3a","metadata":{"id":"7f6c7a3a"},"source":["## What is a Distribution?\n","\n","Imagine you are measuring the height of every dog in a large park. You wouldn't expect every dog to be the exact same height. You'd find a variety of heights: some small dogs, many medium-sized dogs, and some large dogs.\n","\n","A **distribution** is a way of showing all the possible values in your data (the different heights) and how often each value occurs (how many dogs of each height). It gives you a complete picture of the data's personality. The most common way to see a distribution is by drawing a **histogram**."]},{"cell_type":"code","execution_count":null,"id":"5c0bb838","metadata":{"id":"5c0bb838"},"outputs":[],"source":["# Let's simulate the heights (in cm) of 500 dogs\n","dog_heights = np.random.normal(loc=50, scale=15, size=500)\n","\n","plt.figure(figsize=(10, 6))\n","sns.histplot(dog_heights, kde=True, bins=20)\n","plt.title('Distribution of Dog Heights', fontsize=16)\n","plt.xlabel('Height (cm)')\n","plt.ylabel('Number of Dogs (Frequency)')\n","plt.grid(True, linestyle='--', alpha=0.6)\n","plt.show()"]},{"cell_type":"markdown","id":"9243ef00","metadata":{"id":"9243ef00"},"source":["Look at the histogram above. The bars show how many dogs fall into each height range. The smooth line **(called a Kernel Density Estimate or KDE)** gives us a cleaner idea of the overall shape. This specific shape, which is very common, is what we're here to learn about!\n","\n","A density curve is essentially a smoothed-out version of a histogram. Think of the histogram as a blocky, pixelated image of your data's shape; the density curve is the smooth, high-resolution version.\n","**The key change is the y-axis**. Instead of showing counts (how many), the density curve shows probability density. This is done so the total area under the curve is exactly 1, representing 100% of your data, which allows us to think about the area in a certain range as a probability."]},{"cell_type":"code","execution_count":null,"id":"89b18978","metadata":{"id":"89b18978"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","sns.kdeplot(dog_heights)\n","plt.title('Distribution of Dog Heights', fontsize=16)\n","plt.xlabel('Height (cm)')\n","plt.ylabel('Density')\n","plt.grid(True, linestyle='--', alpha=0.6)\n","plt.show()"]},{"cell_type":"markdown","id":"2a2a5fdd","metadata":{"id":"2a2a5fdd"},"source":["## The Normal Distribution: The \"Bell Curve\"\n","\n","The shape we saw with the dog heights is called a **Normal Distribution**, also known as a **Gaussian Distribution** or, most famously, the **\"Bell Curve\"**. It's arguably the most important distribution in all of statistics because it shows up *everywhere* in the real world:\n","- People's heights, weights, and blood pressure\n","- Scores on a test\n","- Measurement errors in an experiment\n","\n","The core idea is simple: **most values cluster around a central average, and values that are further away from the average become less and less common.** In our example, most dogs have a height near the average (50 cm), and there are very few tiny dogs or giant dogs."]},{"cell_type":"markdown","id":"c8ec667a","metadata":{"id":"c8ec667a"},"source":["### Key Characteristics of a Normal Distribution\n","\n","1.  **Symmetrical**: The left side is a perfect mirror image of the right side.\n","2.  **Unimodal**: It has only one peak, or highest point.\n","3.  **Mean, Median, and Mode are Equal**: The center of the curve is the peak, and this single point represents the mean, the median, and the mode of the data.\n","\n","A normal distribution is defined by just two numbers, or **parameters**:\n","- **The Mean ($ \\mu $)**: This tells us where the center of the bell is. It's the peak of the curve.\n","- **The Standard Deviation ($ \\sigma $)**: This tells us how spread out the bell is. A small standard deviation results in a tall, narrow curve, while a large standard deviation results in a short, wide curve."]},{"cell_type":"code","execution_count":null,"id":"0048be90","metadata":{"id":"0048be90"},"outputs":[],"source":["from scipy.stats import norm\n","\n","x = np.linspace(-10, 10, 200)\n","\n","# Same mean, different standard deviations\n","plt.figure(figsize=(12, 6))\n","plt.plot(x, norm.pdf(x, loc=0, scale=1), label='Mean=0, Std Dev=1 (Narrow)')\n","plt.plot(x, norm.pdf(x, loc=4, scale=2), label='Mean=4, Std Dev=2 (Medium)')\n","plt.plot(x, norm.pdf(x, loc=-2, scale=4), label='Mean=-2, Std Dev=4 (Wide)')\n","plt.title('Normal Distributions with Different Standard Deviations', fontsize=16)\n","plt.xlabel('Value')\n","plt.ylabel('Probability Density')\n","plt.legend()\n","plt.grid(True, linestyle='--', alpha=0.6)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"6eca118c","metadata":{"id":"6eca118c"},"outputs":[],"source":["x = np.linspace(120, 220, 200) # Human Height Distribution\n","\n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(x, norm.pdf(x, loc=170, scale=10))\n","plt.title('Normal Distributions with Different Standard Deviations', fontsize=16)\n","plt.xlabel('Value')\n","plt.ylabel('Probability Density')\n","plt.grid(True, linestyle='--', alpha=0.6)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"03fc2de3","metadata":{"id":"03fc2de3"},"outputs":[],"source":["## create your own normal distribution of any interesting feature you'd like"]},{"cell_type":"code","execution_count":null,"id":"36ca25b5","metadata":{"id":"36ca25b5"},"outputs":[],"source":["# Another way to generate normal distribution using numpy\n","\n","data = np.random.normal(loc=170, scale=10, size=1000)\n","\n","# Plot histogram\n","sns.histplot(data, kde=True)\n","plt.title(f'Normal Distribution (mean={data.mean()}, std={data.std()})')\n","plt.xlabel('Value')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"markdown","id":"3e31ad0c","metadata":{"id":"3e31ad0c"},"source":["### The Empirical Rule (The 68-95-99.7 Rule)\n","\n","The Empirical Rule is a fantastic rule of thumb for understanding normally distributed data. It tells us what percentage of our data we can expect to find within a certain number of standard deviations from the mean.\n","\n","For any normal distribution:\n","- **~68%** of the data falls within **1** standard deviation of the mean ($ \\mu \\pm 1\\sigma $).\n","- **~95%** of the data falls within **2** standard deviations of the mean ($ \\mu \\pm 2\\sigma $).\n","- **~99.7%** of the data falls within **3** standard deviations of the mean ($ \\mu \\pm 3\\sigma $).\n","\n","So, if we go back to our dog height example (mean=50, std dev=15):\n","- We can be confident that about 68% of the dogs are between 35 cm ($50-15$) and 65 cm ($50+15$).\n","- We can be very confident that about 95% of the dogs are between 20 cm ($50-30$) and 80 cm ($50+30$).\n","- Almost all the dogs (99.7%) will be between 5 cm and 95 cm."]},{"cell_type":"code","execution_count":null,"id":"5ad83d0f","metadata":{"id":"5ad83d0f"},"outputs":[],"source":["# Visualization of the Empirical Rule\n","mu, sigma = 0, 1 # A standard normal distribution\n","x = np.linspace(mu - 4*sigma, mu + 4*sigma, 500)\n","y = norm.pdf(x, mu, sigma)\n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(x, y, color='black')\n","\n","# Shade areas\n","plt.fill_between(x, y, where=((x > mu - sigma) & (x < mu + sigma)), color='skyblue', alpha=0.5, label='68%')\n","plt.fill_between(x, y, where=((x > mu - 2*sigma) & (x < mu + 2*sigma)), color='dodgerblue', alpha=0.3, label='95%')\n","plt.fill_between(x, y, where=((x > mu - 3*sigma) & (x < mu + 3*sigma)), color='royalblue', alpha=0.2, label='99.7%')\n","\n","plt.title('The Empirical Rule (68-95-99.7)', fontsize=16)\n","plt.xticks([mu-3*sigma, mu-2*sigma, mu-sigma, mu, mu+sigma, mu+2*sigma, mu+3*sigma],\n","           ['μ-3σ', 'μ-2σ', 'μ-1σ', 'μ', 'μ+1σ', 'μ+2σ', 'μ+3σ'])\n","plt.ylabel('Probability Density')\n","plt.grid(True, linestyle='--', alpha=0.5)\n","plt.show()"]},{"cell_type":"markdown","id":"46547baa","metadata":{"id":"46547baa"},"source":["##  Skewness and Kurtosis - Definition and Intuition\n","\n","\n","**Skewness** measures the **asymmetry** or lean of a data distribution. While a perfect normal distribution is perfectly symmetrical (skewness = 0), many real-world datasets are not. A classic example of this is the **Log-Normal Distribution**."]},{"cell_type":"markdown","id":"9b89b0fe","metadata":{"id":"9b89b0fe"},"source":["\n","A variable is log-normally distributed if the **logarithm** of its values is normally distributed. This creates a distribution with a **positive (or right) skew**. The data is bunched up at the low end and has a long tail stretching out to the right.\n","\n","This shape is incredibly common in the real world:\n","- Personal income\n","- Stock prices\n","- The number of comments on a social media post\n","\n","In these cases, most values are modest (most people have an average income), but a few are exceptionally high (a few billionaires), pulling the **mean** far to the right of the **median**."]},{"cell_type":"code","execution_count":null,"id":"12663fce","metadata":{"id":"12663fce"},"outputs":[],"source":["from scipy.stats import skew, kurtosis"]},{"cell_type":"code","execution_count":null,"id":"3bc502e6","metadata":{"id":"3bc502e6"},"outputs":[],"source":["# 1. Right-skewed distribution (positive skew, long tail on the right)\n","data_right_skewed = np.random.lognormal(mean=1, sigma=1, size=1000)\n","\n","# 2. Left-skewed distribution (negative skew, long tail on the left)\n","data_left_skewed = -np.random.lognormal(mean=1, sigma=1, size=1000)\n","\n","# 3. Normal distribution (no skew)\n","data_normal = np.random.normal(loc=0, scale=1, size=1000)\n","\n","# # Calculate skewness for each distribution\n","# skew_right = pd.Series(data_right_skewed).skew()\n","# skew_left = pd.Series(data_left_skewed).skew()\n","# skew_normal = pd.Series(data_normal).skew()\n","\n","skew_right = skew(data_right_skewed)\n","skew_left = skew(data_left_skewed)\n","skew_normal = skew(data_normal)\n","\n","# Print skewness values\n","print(\"Skewness for right-skewed distribution:\", skew_right)\n","print(\"Skewness for left-skewed distribution:\", skew_left)\n","print(\"Skewness for normal distribution:\", skew_normal)\n","\n","# Plot all three distributions in one figure\n","fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n","\n","# Right-skewed plot\n","axes[0].hist(data_right_skewed, bins=30, edgecolor='black', color='lightcoral')\n","axes[0].set_title(f'Right-Skewed Distribution\\n(Skewness = {skew_right:.2f})')\n","axes[0].set_xlabel('Values')\n","axes[0].set_ylabel('Frequency')\n","\n","# Left-skewed plot\n","axes[1].hist(data_left_skewed, bins=30, edgecolor='black', color='lightgreen')\n","axes[1].set_title(f'Left-Skewed Distribution\\n(Skewness = {skew_left:.2f})')\n","axes[1].set_xlabel('Values')\n","axes[1].set_ylabel('Frequency')\n","\n","# Normal distribution plot\n","axes[2].hist(data_normal, bins=30, edgecolor='black', color='lightblue')\n","axes[2].set_title(f'Normal Distribution\\n(Skewness = {skew_normal:.2f})')\n","axes[2].set_xlabel('Values')\n","axes[2].set_ylabel('Frequency')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","id":"1e472a07","metadata":{"id":"1e472a07"},"source":["### The Power of Transformation\n","The beauty of a log-normal distribution is that we can easily 'fix' the skew. By taking the natural logarithm (`np.log()`) of the data, we can transform it back to the symmetrical normal distribution it came from. This is a very common and powerful technique in data analysis"]},{"cell_type":"code","execution_count":null,"id":"c5e58bd3","metadata":{"id":"c5e58bd3"},"outputs":[],"source":["# Transform the data with a log function\n","transformed_data = np.log(data_right_skewed)\n","\n","# Visualize the now-symmetrical data\n","plt.figure(figsize=(8, 5))\n","sns.histplot(transformed_data, kde=True, color='skyblue')\n","plt.title(f'Data After Log Transformation (Skewness: {stats.skew(transformed_data):.2f})')\n","plt.xlabel('Log(Value)')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"930fde72","metadata":{"id":"930fde72"},"outputs":[],"source":["### How to do the same transformation to the left??? ###"]},{"cell_type":"markdown","id":"59d03b81","metadata":{"id":"59d03b81"},"source":["## Kurtosis: How Heavy Are the Tails?\n","\n","**Kurtosis** measures the \"tailedness\" of a distribution. It tells you how much of the data is in the tails compared to a normal distribution. This is a great way to spot the presence of **outliers**.\n","\n","We measure kurtosis relative to the normal distribution (which has a kurtosis of 0).\n","\n","#### Positive Kurtosis\n","A distribution with **heavy tails** and a sharper peak. This means it produces more outliers than a normal distribution.\n","* **Kurtosis > 0**\n","\n","#### Negative Kurtosis\n","A distribution with **light tails** and a flatter peak. This means it produces fewer outliers than a normal distribution.\n","* **Kurtosis < 0**\n","\n","#### very low or zero Kurtosis\n","This is a normal distribution, which serves as our baseline.\n","* **Kurtosis = 0**"]},{"cell_type":"code","execution_count":null,"id":"a8942c02","metadata":{"id":"a8942c02"},"outputs":[],"source":["# Generate data with different kurtosis\n","very_centered_data = np.concatenate([np.random.normal(0, 1, 1000), np.random.normal(0, 50, 50)])\n","normal_data = np.random.normal(0, 1.3, 1000)\n","uniform_data = np.random.uniform(-4, 4, 1000)\n","\n","# Plotting\n","fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n","sns.histplot(uniform_data, kde=True, ax=ax1, color='gold')\n","ax1.set_title(f'Neagtive (Kurtosis: {kurtosis(uniform_data):.2f})')\n","sns.histplot(normal_data, kde=True, ax=ax2, color='skyblue')\n","ax2.set_title(f'Low (Kurtosis: {kurtosis(normal_data):.2f})')\n","sns.histplot(very_centered_data, kde=True, ax=ax3, color='violet')\n","ax3.set_title(f'Positive (Kurtosis: {kurtosis(very_centered_data):.2f})')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"c4b79a08","metadata":{"id":"c4b79a08"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"0735f456","metadata":{"id":"0735f456"},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python (kaggle-env)","language":"python","name":"kaggle-env-python311"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}